\documentclass[%
 %twocolumn,
 %preprint,
 onecolumn,
 amsmath, amssymb, aps, pra, 10pt
]{revtex4-2}
\usepackage{amsmath}
\usepackage{appendix}
\usepackage[colorlinks,citecolor=blue,urlcolor=black,bookmarks=false,hypertexnames=true]{hyperref} 
\begin{document}
\title{SCV Fractional Calculus}% Force line breaks with \
\author{Luke A. Siemens}
\email{luke.siemens@lsiemens.com}
\noaffiliation
\date{\today}
\maketitle

I have found a general description of variable order fractional derivatives. I will show that this description applies to any variable order fractional derivative provided it maps analytic functions to analytic functions and that they are analytic with respect to the order of differentiation. I will make the simplifying assumption that functions are analytic, that the domain of the functions is either $\mathbb{C}$ or $\mathbb{C} \times \mathbb{C}$, that they are analytic in the neighborhood of the origin and that they are analytic almost everywhere (in a measure theory sense). Denoting the set of all analytic functions on the domain $\mathbb{C} \times \mathbb{C}$ as $\mathcal{O}(\mathbb{C}^2)$. Let us define a function space $\mathbb{S}$ as the set,

\begin{equation}
\mathbb{S} = \left\lbrace f(x, a) \in \mathcal{O}(\mathbb{C}^2) \middle| \partial_x f(x, a) = f(x, a - 1) \right\rbrace
\label{differentiable_set}
\end{equation}

define the operator $J^{\alpha}$ acting on elements of the set $\mathbb{S}$ as,

\begin{equation}
J^{\alpha} f(x, a) = \Delta_{a}^{\alpha} f(x, a) = f(x, a - \alpha)
\label{fractional_derivative}
\end{equation}

where $\Delta_{a}^{\alpha}$ is an operator shifting the variable $a$ by the amount $\alpha$. Note that if $f(x, a) \in \mathbb{S}$ then $J^{\alpha} f(x, a) \in \mathbb{S}$. I will now show that, this operator when acting on functions in the function space $\mathbb{S}$ satisfies the necessary properties to be considered a variable order fractional derivative and it is equivalent to any sufficiently analytic variable order fractional derivative on some subspace of $\mathbb{S}$.
I will use the criterion, $\,^3P$, given in your paper \textit{What is a fractional derivative} \cite{ORTIGUEIRA20154} to determine if the operator \eqref{fractional_derivative} is a fractional derivative when acting on functions in the set \eqref{differentiable_set}. In the following arguments let $f(x, a), g(x, a) \in \mathbb{S}$ and $C_1, C_2, \alpha, \beta \in \mathbb{C}$.

\subsection*{$\,^3P1$ : linearity}
Since the shift operator $\Delta_{a}^{\alpha}$ is a linear operator then the operator $J^{\alpha}$, defined in equation \eqref{fractional_derivative}, is also a linear operator.

\subsection*{$\,^3P2$ : Identity}
Using the definition of $J^{\alpha}$ for $\alpha=0$ acting on $f(x, a)$ results in, $J^{0}f(x, a) = \Delta_{a}^{0}f(x, a) = f(x, a)$. So properties $\,^3P2$ is satisfied.

\subsection*{$\,^3P3$ : Backwards compatibility}
In the case where $\alpha$ is a negative integer we can apply $\partial_x f(x, a) = f(x, a - 1)$ repeatedly. Let $\alpha = -k$ with $k \in \mathbb{Z}^+$, $J^{-k} f(x, a) = \Delta_{a}^{-k} f(x, a)=\Delta_{a}^{-k + 1}\Delta_{a}^{+1} f(x, a) = \Delta{a}^{-k + 1}f(x, a - 1) = \Delta_{a}^{-k + 1}\partial_x f(x, a)$, Since $\partial_x f(x, a) \in \mathbb{S}$ we can repeat this argument, so $J^{-k} f(x, a) = \partial_{x}^{-k} f(x, a)$. For the positive case first let us try $k=1$. In this case $J^{1} f(x, a) = f(x, a + 1)$, so $\partial_x f(x, a + 1) = f(x, a)$ leading to the solution $J^{1} f(x, a) = \int_{x_0}^{x} f(t, a)dt + f(x_0, a + 1)$. Provided that $f(x, a)$ is an entire function for all $a$, then applying this repeatedly results in $J^{k} f(x, a) = \frac{1}{\Gamma(k)}\int_{x_0}^{x} (x - t)^{k - 1}f(t, a)dt + \sum_{i = 0}^{k - 1}f(x_0, a + k - i)\frac{x^i}{i!}$. So if $\alpha \in \mathbb{Z}$ then $J^{\alpha}$ represents either repeated integration or differentiation.

\subsection*{$\,^3P4$ : Index law}
Applying fractional derivative twice and simplifying, $J^{\beta}J^{\alpha} f(x, a) = \Delta{a}^{\beta}\Delta_{a}^{\alpha} f(x, a) = \Delta_{a}^{\beta} f(x, a + \alpha) = f(x, a + \alpha + \beta) = J^{\beta + \alpha} f(x, a)$. \\

The criterion $\,^3P1 - \,^3P4$ are satisfied, but before addressing criterion $\,^3P5$ I show that any sufficiently analytic variable order fractional derivative is equivalent to the operator \eqref{fractional_derivative} acting on some subspace of the set $\mathbb{S}$. Now let $J'^{\alpha}$ be an arbitrary variable order fractional derivative that satisfied $\,^3P1 - \,^3P4$ and that there exists a set $\mathbb{S}''$ of analytic functions on which the operator $J'^{\alpha}$ satisfies $\,^3P1 - \,^3P4$ and where $\forall f(x) \in \mathbb{S}'', J'^{\alpha}f(x) \in \mathcal{O}(\mathbb{C}^2)$. The action of $J'^{\alpha}$ acting on $f(x) \in \mathbb{S}''$ can be expressed as $F(x, a) = J'^{\alpha} f(x)$. Using this let $\mathbb{S}' = \left\lbrace f(x, a) \in \mathcal{O}(\mathbb{C}^2) \middle| \exists f(x) \in \mathbb{S}'', f(x, a) = J'^{\alpha} f(x) \right\rbrace$. Now $J'^{-1}f(x, a) = \partial_x f(x, a)$ by $\,^3P3$ and $J'^{-1} f(x, a) = J'^{-1}J'^{\alpha} f(x) = J'^{\alpha - 1} f(x) = f(x, \alpha - 1)$ by $\,^3P4$, so $\forall f(x, a) \in \mathbb{S}', \partial_x f(x, a) = f(x, a - 1)$. Now for $f(x, a) \in \mathbb{S}'$ taking a fractional derivative $J'^{\alpha} f(x, a) = J'^{\alpha}J'^{a} f(x)$, using $\,^3P4$, $J'^{\alpha} f(x, a) = f(x, a + \alpha) = \Delta_{a}^{\alpha} f(x, a)$. Clearly $\mathbb{S}' \subset \mathbb{S}$ and $\forall f(x, a) \in \mathbb{S}', J'^{\alpha} f(x, a) = J^{\alpha} f(x, a)$. So every variable order fractional derivative which is sufficiently analytic is equivalent to $J^{\alpha}$ acting on some subspace of $\mathbb{S}$. So the fractional derivative defined in \eqref{fractional_derivative} when acting on functions in the space \eqref{differentiable_set} provides a general description of all sufficiently analytic variable order fractional derivatives subject to $\,^3P1 - \,^3P4$.

So far I have used the equation $\partial_x f(x, a) = f(x, a - 1)$ to determine if a function is in the set $\mathbb{S}$ but have not considered whether or not this equation holds on the entire domain of $f(x, a)$ or only on some subset. Because we are only considering analytic functions this equation holds every where or almost no where. Since given $f(x, a) \in \mathcal{O}(\mathbb{C}^2)$ we can define $g(x, a) = \partial_x f(x, a) - f(x, a - 1)$, note that if $f(x, a) \in \mathcal{O}(\mathbb{C}^2)$ then $g(x, a) \in \mathcal{O}(\mathbb{C}^2)$ and that if $\partial_x f(x, a) = f(x, a - 1)$ then $g(x, a) = 0$. Because $g(x, a)$ is a complex analytic function it can be analytically continued such that it is either zero every where or it is zero almost nowhere. Therefore $\partial_x f(x, a) = f(x, a - 1)$ must either be satisfied on the entire domain of $f(x, a)$ or satisfied almost nowhere since $f(x, a)$ is complex analytic.

\subsection*{$\,^3P5$ : Generalized Leibniz rule}
So far I have not found a consistent way to define multiplication in general. If $f(x, a), g(x, a) \in \mathbb{S}$ and $h(x, a) = f(x, a) \cdot g(x, a)$ then $\partial_x h(x, a) = f(x, a - 1) \cdot g(x, a) + f(x, a) \cdot g(x, a - 1) \neq h(x, a - 1)$ unless either $f(x, a)$ or $g(x, a)$ is a constant function. So if multiplication by non-constant functions is possible then the product operator needs to be modified, and due to $\,^3P3$ it needs to be compatible with the General Leibniz rule. In the simpler case of multiplication by an analytic function of one variable $g(x) \in \mathcal{O}(\mathbb{C})$ then a solution is to use the a modification of the equation given in $\,^3P5$. Given $f(x, a) \in \mathcal{O}(\mathbb{C}^2)$ and $g(x) \in \mathcal{O}(\mathbb{C})$ then define multiplication denoted by the symbol $*$ as

\begin{equation}
f(x, a) * g(x) = \sum_{k=0}^{\infty} \binom{-a}{k}f(x, a_0 + a + k) \cdot \frac{d^k}{dx^k} g(x)
\label{multiplication}
\end{equation}

Let $h(x, a) = f(x, a) * g(x)$, and take the partial derivative of $h(x, a)$ with respect to $x$,

$$\partial_x h(x, a) = \sum_{k=0}^{\infty} \binom{-a}{k}f(x, a_0 + a + k - 1) \cdot \frac{d^k}{dx^k} g(x) + \binom{-a}{k}f(x, a_0 + a + k) \cdot \frac{d^{k + 1}}{dx^{k + 1}} g(x)$$

Using $k = k' - 1$ in the second term and $k = k'$ in the first and since $\binom{-a}{k-1}$ is zero if $k=0$, we can rearrange to find

$$\partial_x h(x, a) = \sum_{k'=0}^{\infty} \left( \binom{-a}{k'} + \binom{-a}{k' - 1} \right)f(x, a_0 + (a - 1) + k') \cdot \frac{d^{k'}}{dx^{k'}} g(x)$$

Now using the fact that $\binom{-a}{k} + \binom{-a}{k - 1} = \binom{-a + 1}{k}$ and the definition of $h(x, a)$, then 

$$\partial_x h(x, a) = \sum_{k'=0}^{\infty} \binom{-(a - 1)}{k'}f(x, a_0 + (a - 1) + k') \cdot \frac{d^{k'}}{dx^{k'}} g(x) = h(x, a - 1)$$

So if $f(x, a) \in \mathbb{S}$ and $g(x) \in \mathcal{O}(\mathbb{C})$ then $f(x, a) * g(x) \in \mathbb{S}$, if it exists, and clearly $f(x, a) * g(x)$ satisfies $\,^3P5$. Currently I am working on finding an operator that naturally generalizes multiplication that works when both functions are in the set $\mathbb{S}$.

\section*{PDE representation}
We can construct an alternative for of $\partial_x f(x, a) = f(x, a - 1)$ by using the definition of the shift operator $\Delta_{x}^{t} = e^{t \partial_x}$. So expanding the exponential function is this definition $\Delta_{x}^t = \sum_{k=0}^{\infty} \frac{t^k}{k!} \partial_{x}^{k}$, which allows us to express $\partial_x f(x, a) = f(x, a - 1)$ as the PDE $\partial_x f(x, a) - \sum_{k=0}^{\infty} \frac{(-1)^k}{k!}\partial_{a}^{k} f(x, a) = 0$. So we can solve for elements of $\mathbb{S}$ by solving the system of PDEs $\partial_x f(x, a) - \sum_{k=0}^{\infty} \frac{(-1)^k}{k!} \partial_{a}^{k} f(x, a) = 0$, $\partial_{\bar{x}} f(x, a) = 0$, $\partial_{\bar{a}} f(x, a) = 0$ where the last two PDEs ensure that $f(x, a)$ is complex analytic.

\section*{Series solution}
Given $f(x, a) \in \mathcal{O}(\mathbb{C}^2)$ it can be described by the power series $f(x, a) = \sum_{j=0}^{\infty} \sum_{k=0}^{\infty} C_{j, k} \frac{x^j}{j!} \frac{a^k}{k!}$. we can rearrange the series to  the form $f(x, a) = \sum_{j=0}^{\infty} \frac{x^j}{j!} \sum_{k=0}^{\infty} C_{j, k} \frac{a^k}{k!}$, and define $g_n(a) = \sum_{k=0}^{\infty} C_{n, k} \frac{a^k}{k!}$, so that $f(x, a) = \sum_{j=0}^{\infty} \frac{x^j}{j!} g_j(a)$, if $g_n(a)$ converges for all $n \in \mathbb{Z}^+$. If $f(x, a) \in \mathbb{S}$ then $\partial_x f(x, a) = \sum_{j=0}^{\infty} \sum_{k=0}^{\infty} C_{j + 1, k} \frac{x^j}{j!} \frac{a^k}{k!} = \sum_{j=0}^{\infty} \sum_{k=0}^{\infty} C_{j, k} \frac{x^j}{j!} \frac{(a - 1)^k}{k!}$. Taking the nth partial derivative of $x$ on both sides and letting $x=0$ yields, $\sum_{k=0}^{\infty} C_{n + 1, k} \frac{a^k}{k!} = \sum_{k=0}^{\infty} C_{n, k} \frac{(a -1)^k}{k!}$. So then using the definition of $g_n(a)$ in the previous equation $g_{n+1}(a) = g_{n}(a - 1)$ and applying this repeatedly gives the equation, $g_{n}(a) = g_{0}(a - n)$. so if $f(x, a) \in \mathbb{S}$ then $f(x, a) = \sum_{j=0}^{\infty} \frac{x^j}{j!} g_{j}(a) = \sum_{j=0}^{\infty} \frac{x^j}{j!} g_{0}(a - j)$, thus for any $g(a) \in \mathcal{O}(\mathbb{C})$ if $f(x, a) = \sum_{j=0}^{\infty} \frac{x^j}{j!} g(a - j)$ converges then $f(x, a) \in \mathbb{S}$. Note that $g_{0}(a) = f(0, a)$, so if $f(x, a) \in \mathbb{S}$ then $f(x, a) = \sum_{j=0}^{\infty} \frac{x^j}{j!} f(0, a)$ and for entire functions $f(x, a)$ converges for all finite $x$ and $a$. Currently I am looking into using this idea to construct an alternative representation of the fractional calculus I have described in this document, as it looks like it might be a more natural description.

\bibliographystyle{plain}
\bibliography{scv_fractional_calculus.bib}
\end{document}
