<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[I-Process]]></title><description><![CDATA[Thoughts and Ideas on Physics, Astronomy and Computer Science.]]></description><link>http://lsiemens.github.io/</link><generator>Ghost v0.4.2</generator><lastBuildDate>Sat, 14 Jun 2014 22:36:27 GMT</lastBuildDate><atom:link href="http://lsiemens.github.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[What is Momentum?]]></title><description><![CDATA[<p>When you took your first physics class you where introduced to the concept of momentum, and the discussion likely revolved around Newton's second law of motion. This discussion would then culminate in the definition of momentum: $\vec{p} = m \vec{v}$. As you progressed in the sciences you learned more about momentum and why it is useful, one of the most important properties that you learned about momentum is that, momentum is conserved in all inertial reference frames when the system is closed. When the discussion of classical mechanics concluded you knew that momentum <strong>is</strong> $m \vec{v}$ and that it is conserved.</p>

<p>At some point later on you learned about special relativity, and that in the early twentieth century Einstein postulated that the speed of light was constant for all reference frames. In order for this to be consistent he needed to replace the Galilean transformation with a new set of equations for transforming between reference frames, these new equations are called the Lorentz transformations. But under the new transformations the quantity $m \vec{v}$ was no longer conserved in all inertial reference frames. So he revised the equation for momentum, the new equation he found was $\vec{p} = \gamma m_{0} \vec{v}$. This new equation for momentum results in momentum being conserved for all inertial reference frames if the system is closed, and for velocities that are small relative to the speed of light the equation reduces down to $m \vec{v}$. So as it turned out, $m \vec{v}$ was a good approximation of momentum the velocity is small, but real equation for momentum is the one discovered by Einstein. By the time that the discussion of special relativity was over you knew that momentum <strong>is</strong> $\gamma m_{0} \vec{v}$.</p>

<p>Unfortunately both of these definition are wrong. Momentum is not $\gamma m_{0} \vec{v}$ and it is most definitely not $m \vec{v}$. Fundamentally there is a symmetry in the universe involving mass and velocity and the quantity that satisfies this symmetry for all reference frames in a closed system <strong>is</strong> momentum. The quantity $m \vec{v}$ is called momentum because it has been experimentally show to approximate the momentum when the velocity is small, and $\gamma m_{0} \vec{v}$ is called momentum because it has similarly been shown to approximate the true value of momentum. Currently the best equation for momentum is the one discovered by Einstein, but that equation for momentum may or may not be superseded by a more general equation. We call the results of these equations momentum because they are good approximations under particular constraints. Momentum is the quantity that satisfies this particular symmetry not the equations that approximate it.</p>]]></description><link>http://lsiemens.github.io/what-is-momentum/</link><guid isPermaLink="false">7c14b468-6784-481b-9790-12ea9b79b4ee</guid><category><![CDATA[Physics]]></category><category><![CDATA[Special Relativity]]></category><dc:creator><![CDATA[Luke Siemens]]></dc:creator><pubDate>Fri, 13 Jun 2014 17:11:26 GMT</pubDate></item><item><title><![CDATA[An Introduction to Markov Chains]]></title><description><![CDATA[<p>Markov chains are a statistical tool invented by Andrey Markov to model dependent statistical phenomenon. A Markov Chain is made from two components a discrete set of states and a set of stochastic state transitions. The idea is that if there are two states <strong>A</strong> and <strong>B</strong> and the state transitions <strong>AA</strong>, <strong>AB</strong>, <strong>BA</strong> and <strong>BB</strong> where the first letter is the starting state and the second letter is the final state, each one of these state transitions will have a probability of occurring associated with it. To compute this process an initial state is selected, then one of the valid state transitions is randomly chosen based on its probability of occurring, the current state is replaced by the new state as determined by the transition. This process now is a model of a system where the probability of the states <strong>A</strong> or <strong>B</strong> occurring is dependent on the current state.</p>

<p>The example given was quite simple and would not show much complexity, but it is possible to extend a Markov Chain to model more complex systems. If the state transitions of a Markov Chain where to take into account more than the most recent state. Then the chain could model more complexity system. Using the same example as before if an initial sequence of two states where chosen and the state transitions where replaced by the new set of transitions; <strong>AAA</strong>, <strong>AAB</strong>, <strong>ABA</strong>, <strong>ABB</strong>, <strong>BAA</strong>, <strong>BAB</strong>, <strong>BBA</strong> and <strong>BBB</strong> then this process could model more complicated systems. So the complexity of the resulting sequence is determined by the number of previous states accounted for in the state transitions, this is called the order of the Markov Chain. The first example was a first order chain, the second was a second order chain. So the simplest Markov Chain is a zeroth order chain where none of the previous states are taken into account and the probability of a state is equal regardless of what came before.</p>

<p>To demonstrate how the complexity of a Markov chain is related to the order of the chain I wrote a program to generate Markov chains. This program uses all of the lower case letters in the English alphabet and the space and period characters as states of the system. A set of state transitions is generated from the text supplied to the program, then the program generates a string of characters using the state transitions and an initial state. For this example I used some of the books and collections of stories written by H. G. Wells including; <em>The First Men in the Moon</em>, <em>The Island of Doctor Moreau</em>, <em>The Time Machine</em>, <em>The War of The Worlds</em>, and <em>The Food of The Gods and How it came to Earth</em>. These books were downloaded for free from the <a href='http://www.gutenberg.org/' >Gutenberg Project</a>.</p>

<p>After running four tests using zeroth, first, second and fifth order Markov chains the resulting text output was as follows. The zeroth order Markov chain produced the text,</p>

<blockquote>
  <p>icehciotsi  b  n   o e ble plwdenvehtp axeuiod a oeoht  a ue  utf u mmn  aauuea rptge tn hd mo  ehd. hial aald.ued p rliuryudwfoepulc  .aoyvnhimwomo   m e aeaiyofeof  rtah  i ode rar yk otss  ogtghhdglawmeet e hreetcoedd a ms.r u ochcirp e rodihfrca whocroegglnneeos .sahr hwi ttliepcn liso mou tdn   eaaoaifeno.fkeeoeaaii f  nl   t eh s eil   a tblanlo ahseuefeuor tomgtsivhesnrdh yoeiate edwotd  hpoago uha been rafey.l wl  yh oeebaumdihlouctrtos afcso  th rkgpttwgnsneeerai    inhreeh nneenh pifee</p>
</blockquote>

<p>In this sample text the characters appear at a frequency that is consistent with the books written by H. G. Wells, but it looks nothing like English and has no structure. The first order Markov chain produces the text,</p>

<blockquote>
  <p>atht gsond sly ig. of tiroveratustu tl s as thengrd wif ly by s . thoulel s therdad  coaneree ithend cthe i ct w ltr agar mathe f ot ds o abos sor ild witlf ano wit ovesth he the trhercange acuang bof thed intos. hroge  ayenxther o st bera wisem lyotherictouringa og ve bevuledrsers f creled. t bowhe woruan wead tind mm sury blay her qut k the frk theye ananererol tongheatracon wit impulacotre basoubyngereasthe ir apawain aikls ld evevelny iseee tiloved athap histhaneg ime ffatenacte waf t ve thu</p>
</blockquote>

<p>This text still does not look like English, but the Markov chain is starting to produce some digraphs that are common to the English language. The second order Markov chain produces the text,</p>

<blockquote>
  <p>as turn the selted thavere led buse clisely daw an.  yelivertyfor full begaidebroblaming mattle ber dark of havou. thatintliver her sand abot he shore sund tows i. to hunnouse.  i doneou attless any dissittely a monse.  and ungsto mand he iten thery sper obweed wit ang ups at cord frome ingethick of fract win th mulf hown selpeople th budd old. he the dance go to rome. to hemand bitiough thin had. talf therses vand thanceenes on threat at sidido  ift lon othe re he sudden mysta must couccum.</p>
</blockquote>

<p>The second order Markov chain is starting to produce some words and produce some basic structure such as ensuring that periods are followed by spaces. Finally the fifth order Markov chain produces the text,</p>

<blockquote>
  <p>as i have died a snakes already to freedom of desire beginning until so furniture staghound with people in it. . . they would heap of human by readingrejoicing done  bogota that the fit to answer to and knew to carried to me strange orchid. its what all was dying and more that said. why the explanation absolutely up their eyes for us. graham was unaccustomersfor sheds it a being course was impossibilities and whistle and probably free other in the opposition but of the sunlightfall over i thing</p>
</blockquote>

<p>This text is clearly unintelligible but at first glance it looks like English, also the trend of the text from the zeroth order to the fifth order chain appears to indicate that the order of the chain is directly related to the complexity of the Markov chain and its ability to mimic complicated systems such as the English language.</p>]]></description><link>http://lsiemens.github.io/an-introduction-to-markov-chains/</link><guid isPermaLink="false">786ebf52-0367-453f-b660-be35f5b11c4c</guid><category><![CDATA[Statistics]]></category><category><![CDATA[Markov Chains]]></category><dc:creator><![CDATA[Luke Siemens]]></dc:creator><pubDate>Sun, 08 Jun 2014 00:28:40 GMT</pubDate></item></channel></rss>